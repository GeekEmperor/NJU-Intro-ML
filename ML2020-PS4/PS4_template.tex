\documentclass[a4paper,UTF8]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{color}
\usepackage{ctex}
\usepackage{enumerate}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tcolorbox}

\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\usepackage{multirow}              

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 12pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2020年春季}                    
\chead{机器学习导论}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业一}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
		\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
		\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
		\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
	\vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%--

%--
\begin{document}
\title{机器学习导论\\习题四}
\author{171830635, 俞星凯, yuxk@smail.nju.edu.cn}
\maketitle


\section*{学术诚信}

本课程非常重视学术诚信规范，助教老师和助教同学将不遗余力地维护作业中的学术诚信规范的建立。希望所有选课学生能够对此予以重视。\footnote{参考尹一通老师\href{http://tcs.nju.edu.cn/wiki/}{高级算法课程}中对学术诚信的说明。}

\begin{tcolorbox}
	\begin{enumerate}
		\item[(1)] 允许同学之间的相互讨论，但是{\color{red}\textbf{署你名字的工作必须由你完成}}，不允许直接照搬任何已有的材料，必须独立完成作业的书写过程;
		\item[(2)] 在完成作业过程中，对他人工作（出版物、互联网资料）中文本的直接照搬（包括原文的直接复制粘贴及语句的简单修改等）都将视为剽窃，剽窃者成绩将被取消。{\color{red}\textbf{对于完成作业中有关键作用的公开资料，应予以明显引用}}；
		\item[(3)] 如果发现作业之间高度相似将被判定为互相抄袭行为，{\color{red}\textbf{抄袭和被抄袭双方的成绩都将被取消}}。因此请主动防止自己的作业被他人抄袭。
	\end{enumerate}
\end{tcolorbox}

\section*{作业提交注意事项}
\begin{tcolorbox}
	\begin{enumerate}
		\item[(1)] 请在LaTeX模板中{\color{red}\textbf{第一页填写个人的姓名、学号、邮箱信息}}；
		\item[(2)] 本次作业需提交该pdf文件、问题4可直接运行的源码(main.py)、问题4的输出文件(学号\_ypred.csv)，将以上三个文件压缩成zip文件后上传。zip文件格式为{\color{red}\textbf{学号.zip}}，例如170000001.zip；pdf文件格式为{\color{red}\textbf{学号\_姓名.pdf}}，例如170000001\_张三.pdf。
		\item[(3)] 未按照要求提交作业，或提交作业格式不正确，将会{\color{red}\textbf{被扣除部分作业分数}}；
		\item[(4)] 本次作业提交截止时间为{\color{red}\textbf{5月14日23:59:59}}。除非有特殊情况（如因病缓交），否则截止时间后不接收作业，本次作业记零分。
	\end{enumerate}
\end{tcolorbox}

\newpage

\section*{\textbf{[30 pts]} Problem 1 [Kernel Functions]}

\begin{enumerate}[(1)]
	\item \textbf{[10 pts]} 对于$\bm{x},\bm{y} \in \mathbb{R}^N$，考虑函数$\kappa(x,y) = \tanh( a \bm{x}^\top \bm{y} + b)$，其中$a,b$是任意实数。试说明$a \geq 0,b \geq 0$是$\kappa$为核函数的必要条件。
	\item \textbf{[10 pts]} 考虑$ \mathbb{R}^N $上的函数$ \kappa(\bm{x},\bm{y}) = (\bm{x}^\top \bm{y} + c)^d $，其中$c$是任意实数，$d,N$是任意正整数。试分析函数$\kappa$何时是核函数，何时不是核函数，并说明理由。
	\item \textbf{[10 pts]} 当上一小问中的函数是核函数时，考虑$d=2$的情况，此时$\kappa$将$N$维数据映射到了什么空间中？具体的映射函数是什么？更一般的，对$d$不加限制时，$\kappa$将$N$维数据映射到了什么空间中？(本小问的最后一问可以只写结果)
\end{enumerate}

\begin{solution}
	~\\
	\begin{enumerate}[(1)]
	\item 
	因为对于任意数据集，核矩阵都是半正定的，所以对于任意$x\in \mathbb{R}^N$，满足$\kappa(x,x)\geq0$，即$ax^Tx+b\geq0$，而$x^Tx\geq0$，所以$a\geq0,b\geq0$，于是$a\geq0,b\geq0$是$\kappa$为核函数的必要条件。
	\item 
	当$c\geq0$时，使用二项式定理可以将$\kappa(x,y)=(x^Ty+c)^d$展开为$n+1$个项的线性组合，并且系数非负。已知$\kappa'(x,y)=(x^Ty)^k$对任意正整数$k$都是核函数，根据核函数的非负线性组合也是核函数，$\kappa$是核函数。\\
	%当$c\geq0$时，令$\hat{x}=(x;\sqrt{c}),\hat{y}=(y;\sqrt{c})$，有$\kappa(x,y)=\kappa(\hat{x},\hat{y})=(\hat{x}^T\hat{y})^d$，所以$\kappa$是核函数。\\
	当$c<0$时，考虑$d$的奇偶性。如果$d$是奇数，根据$\kappa(\bm{0}_N,\bm{0}_N)=c^d<0$，$\kappa$不是核函数。如果$d$是偶数，考虑数据集$D=\{x,y\}$，核矩阵$K$的行列式$det(K)=(x^Tx+c)^d(y^Ty+c)^d-(x^Ty+c)^{2d}$，令$x=\bm{0}_N,y=(\bm{0}_{N-1};\sqrt{-c})$，则$x^Tx=0,x^Ty=0,y^Ty=-c$，于是$det(K)=-c^{2d}<0$，$\kappa$不是核函数。\\
	综上所述，当$c\geq0$时，$\kappa$是核函数；当$c<0$时，$\kappa$不是核函数。
	\item 
	$\kappa$将$N$维数据映射到了$\frac{(N+1)(N+2)}{2}$维空间，映射函数
	\[\kappa(x)=(c,\sqrt{2c}x_1,\sqrt{2c}x_2,\cdots,\sqrt{2c}x_N,x_1^2,x_2^2,\cdots,x_N^2,\sqrt{2}x_1x_2,\sqrt{2}x_1x_3\cdots,\sqrt{2}x_{N-1}x_N)\]
	考虑$\kappa(x,y)=(x^Ty+c)^d$的展开式的任意一项中，$c,x_1y_1,x_2y_2,\cdots,x_Ny_N$的指数分别是$d_0,d_1,d_2,\cdots,d_N$，满足$\sum_{i=0}^{N}d_i=d$，并且$d_i(0\leq i\leq N)$是非负整数。由排列组合知识易知有$C_{N+d}^d$种可能，即$\kappa$将$N$维数据映射到了$C_{N+d}^d$维空间。
	\end{enumerate}
\end{solution}

\section*{[30 pts] Problem 2 [Surrogate Function in SVM]}

在软间隔支持向量机问题中，我们的优化目标为
\begin{equation}\label{eq1}
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right) . 
\end{equation}
然而$\ell_{0 / 1}$数学性质不太好，它非凸、非连续，使得式(\ref{eq1})难以求解。实践中我们通常会将其替换为“替代损失”，替代损失一般是连续的凸函数，且为$\ell_{0 / 1}$的上界，比如hinge损失，指数损失，对率损失。下面我们证明在一定的条件下，这样的替换可以保证最优解不变。

我们考虑实值函数$h:\mathcal{X}\rightarrow\mathbb{R}$构成的假设空间，其对应的二分类器$f_h:\mathcal{X}\rightarrow\{+1,-1\}$为
$$f_{h}(x)=\left\{\begin{array}{ll}
+1 & \text { if } h(x)\geq 0 \\
-1 & \text { if } h(x)<0
\end{array}\right.$$
$h$的期望损失为$R(h)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[I_{f_{h}(x) \neq y}\right]$，其中$I$为指示函数。设$\eta(x)=\mathbb{P}(y=+1|x)$，则贝叶斯最优分类器当$\eta(x)\geq \frac{1}{2}$时输出$1$，否则输出$-1$。因此可以定义贝叶斯得分$h^*(x)=\eta(x)-\frac{1}{2}$和贝叶斯误差$R^*=R(h^*)$。

设$\Phi:\mathbb{R}\rightarrow\mathbb{R}$为非减的凸函数且满足$\forall u\in \mathbb{R},1_{u\leq 0}\leq \Phi(-u)$。对于样本$(x,y)$，定义函数$h$在该样本的$\Phi$-损失为$\Phi(-yh(x))$，则$h$的期望损失为$\mathcal{L}_{\Phi}(h)=\underset{(x, y) \sim \mathcal{D}}{\mathbb{E}}[\Phi(-y h(x))]$。定义$L_{\Phi}(x, u)=\eta(x) \Phi(-u)+(1-\eta(x)) \Phi(u)$，设$h_{\Phi}^{*}(x)=\underset{u \in[-\infty,+\infty]}{\operatorname{argmin}} L_{\Phi}(x, u)$，$\mathcal{L}_{\Phi}^{*}=\mathcal{L}_{\Phi}(h_{\Phi}^{*}(x))$。

我们考虑如下定理的证明：

若对于$\Phi$，存在$s\geq 1$和$c>0$满足对$\forall x\in\mathcal{X}$有
\begin{equation}\label{eq2}
\left|h^{*}(x)\right|^{s}=\left|\eta(x)-\frac{1}{2}\right|^{s} \leq c^{s}\left[L_{\Phi}(x, 0)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right]
\end{equation}
则对于任何假设$h$，有如下不等式成立
\begin{equation}\label{eq3}
R(h)-R^{*} \leq 2 c\left[\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}\right]^{\frac{1}{s}}
\end{equation}

\begin{enumerate}[(1)]
	\item \textbf{[5 pts]} 请证明
	\begin{equation}\label{eq4}
	\Phi\left(-2 h^{*}(x) h(x)\right)\leq L_{\Phi}(x, h(x))
	\end{equation}

	\item \textbf{[10 pts]} 请证明
	\begin{equation}\label{eq5}
	R(h)-R^{*}\leq 2 \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|h^{*}(x)\right| 1_{h(x) h^{*}(x) \leq 0}\right]
	\end{equation}
	提示：先证明
	$$R(h)=\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[2 h^{*}(x) 1_{h(x)<0}+(1-\eta(x))\right]$$
	\item \textbf{[10 pts]} 利用式(\ref{eq4})和式(\ref{eq5})完成定理的证明。
	\item \textbf{[5 pts]} 请验证对于Hinge损失 $\Phi(u)=\max(0,1+u)$，有$s=1,c=\frac{1}{2}$。
\end{enumerate}

\begin{solution}
	~\\
	\begin{enumerate}[(1)]
		\item 
		将$h^*(x)=\eta(x)-\frac{1}{2}$代入$\Phi(-2 h^*(x) h(x))$得
		\[\Phi(-2 h^*(x) h(x))=\Phi((1-2\eta(x)) h(x))=\Phi(-\eta(x)h(x)+(1-\eta(x))h(x))\]
		因为$\Phi$是凸函数，由凸函数的性质得
		\[\Phi(\eta(x)(-h(x))+(1-\eta(x))h(x))\leq\eta(x)\Phi(-h(x))+(1-\eta(x))\Phi(h(x))=L_{\Phi}(x, h(x))\]
		\item
		计算$R(h)$
		\begin{align*}
			R(h)
			&=\mathbb{E}_{(x, y) \sim \mathcal{D}}[I_{f_{h}(x) \neq y}]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[I_{f_{h}(x) \neq+1}\mathbb{P}(y=+1|x)+I_{f_{h}(x) \neq-1}\mathbb{P}(y=-1|x)]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[1_{h(x)<0}\eta(x)+(1-1_{h(x)<0})(1-\eta(x))]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[(2\eta(x)-1) 1_{h(x)<0}+1-\eta(x)]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[2 h^*(x) 1_{h(x)<0}+(1-\eta(x))]\\
		\end{align*}
		将$h^*$代入
		\[R^*=R(h^*)=\mathbb{E}_{x \sim \mathcal{D}_{x}}[2 h^*(x) 1_{h^*(x)<0}+(1-\eta(x))]\]
		所以
		\begin{align*}
			R(h)-R^*
			&=2\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[h^*(x)(1_{h(x)<0}-1_{h^*(x)<0})]\\
			&\leq 2\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[\vert h^*(x)\vert \vert 1_{h(x)<0}-1_{h^*(x)<0}\vert]\\
			&=2\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[\vert h^*(x)\vert 1_{h(x)h^*(x)\leq 0}]
		\end{align*}
		\item 
		计算$\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}$
		\begin{align*}
			\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}
			&=\mathbb{E}_{(x, y) \sim \mathcal{D}}[\Phi(-yh(x))]-\mathbb{E}_{(x, y) \sim \mathcal{D}}[\Phi(-yh_\Phi^*(x))]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[\Phi(-h(x))\eta(x)+\Phi(h(x))(1-\eta(x))]\\
			&\quad+\mathbb{E}_{x \sim \mathcal{D}_{x}}[\Phi(-h_\Phi^*(x))\eta(x)+\Phi(h_\Phi^*(x))(1-\eta(x))]\\
			&=\mathbb{E}_{x \sim \mathcal{D}_{x}}[L_\Phi(x,h(x))-L_\Phi(x,h_\Phi^*(x))]
		\end{align*}
		由式(2)和式(5)得
		\begin{align*}
			R(h)-R^* 
			&\leq 2\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[\vert h^*(x)\vert 1_{h(x)h^*(x)\leq 0}\\
			&\leq 2\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[c[L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))]^{\frac1s} 1_{h(x)h^*(x)\leq 0}]\\
			&=2c\ \mathbb{E}_{x \sim \mathcal{D}_{x}}[[(L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))) 1_{h(x)h^*(x)\leq 0}]^{\frac1s}]
		\end{align*}
		要证式(3)，只要证明
		\[(L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))) 1_{h(x)h^*(x)\leq 0} \leq L_\Phi(x,h(x))-L_\Phi(x,h_\Phi^*(x))\]
		分类讨论，若$h(x)h^*(x)>0$
		\[
		L_\Phi(x,h(x))-L_\Phi(x,h_\Phi^*(x)) \geq L_\Phi(x,h(x))-\min_uL_\Phi(x,u)\geq 0
		\]
		若$h(x)h^*(x)\leq0$
		\begin{align*}
			L_\Phi(x,h(x))-L_\Phi(x,h_\Phi^*(x)) 
			&\geq\Phi(-2 h^{*}(x) h(x))-L_\Phi(x,h_\Phi^*(x))\\
			&\geq\Phi(0)-L_\Phi(x,h_\Phi^*(x))\\
			&=L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))
		\end{align*}
		因此
		\[R(h)-R^{*} \leq 2 c[\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}]^{\frac{1}{s}}\]
		\item
		将$L_\Phi(x,u)$和$\Phi(u)$展开
		\[L_\Phi(x,h_\Phi(x))=\eta(x)\max(0,1-h_\Phi(x))+(1-\eta(x))\max(0,1+h_\Phi(x))\]
		因为$L_\Phi(x,h_\Phi^*(x))$是最小的，下面分段求出$L_\Phi(x,h_\Phi^*(x))$\\
		当$h_\Phi(x)\geq1$时
		\[L_\Phi(x,h_\Phi^*(x))=(1-\eta(x))(1+h_\Phi(x)) \geq 2(1-\eta(x))\]
		当$h_\Phi(x)\leq-1$时
		\[L_\Phi(x,h_\Phi(x))=\eta(x)(1-h_\Phi(x)) \geq 2\eta(x)\]
		当$-1\leq h_\Phi(x) \leq1$时
		\[L_\Phi(x,h_\Phi(x))=\eta(x)(1-h_\Phi(x))+(1-\eta(x))(1+h_\Phi(x))=1+(1-2\eta(x))h_\Phi(x)\]
		若$1-2\eta(x)\geq0$
		\[L_\Phi(x,h_\Phi(x)) \geq 2\eta(x)\]
		若$1-2\eta(x)\leq0$
		\[L_\Phi(x,h_\Phi(x)) \geq 2(1-\eta(x))\]
		因此得到
		\[L_\Phi(x,h_\Phi^*(x))=\min L_\Phi(x,h_\Phi(x))=\min(2\eta(x),2(1-\eta(x)))\]
		所以
		\begin{align*}
			c^{s}[L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))]
			&=\frac12[L_{\Phi}(x, 0)-L_{\Phi}(x, h_{\Phi}^{*}(x))]\\
			&=\frac12[\Phi(0)-\min(2\eta(x),2(1-\eta(x)))]\\
			&=\frac12\max(1-2\eta(x),-1+2\eta(x))\\
			&=\frac12\vert(2\eta(x)-1)\vert\\
			&=\vert\eta(x)-\frac12\vert\\
			&=\vert h^*(x)\vert^s
		\end{align*}
	\end{enumerate}
\end{solution}
\section*{[20 pts] Problem 3 [Generalization Error of SVM]}

留一损失(leave-one-out error)使用留一法对分类器泛化错误率进行估计，即：每次使用一个样本作为测试集，剩余样本作为训练集，最后对所有测试误差求平均。对于SVM算法$\mathcal{A}$，令$h_S$为该算法在训练集$S$上的输出，则该算法的经验留一损失可形式化定义为
\begin{equation}
	\hat{R}_{\text{LOO}}(\mathcal{A}) = \frac{1}{m} \sum_{i=1}^m 1_{ h_{ S-\{x_i\} } (x_i) \neq y_i } . 
\end{equation}
本题通过探索留一损失的一些数学性质，来分析SVM的泛化误差，并给出一个期望意义下的泛化误差界。(注：本题仅考虑可分情形。)

\begin{enumerate}[(1)]
	\item \textbf{[10pts]} 在实践中，测试误差相比于泛化误差是很容易获取的。虽然测试误差不一定是泛化误差的准确估计，但测试误差与泛化误差往往能在期望意义下一致。试证明留一损失满足该性质，即
	\begin{equation}
		\mathbb{E}_{S \sim \mathcal{D}^m} [ \hat{R}_{\text{LOO} }(\mathcal{A}) ] = \mathbb{E}_{S' \sim \mathcal{D}^{m-1}} [ R(h_{S'}) ] . 
	\end{equation}
	\item \textbf{[5 pts]} SVM之所以取名为SVM，是因为其训练结果仅与一部分样本(即支持向量)有关。这一现象可以抽象的表示为，如果$x$不是$h_S$的支持向量，则$h_{S-\{x\}} = h_S$。这一性质在分析误差时有关键作用，考虑如下问题：如果$x$不是$h_S$的支持向量，$h_{S-\{x\}}$会将$x$正确分类吗，为什么？该问题结论的逆否命题是什么？
	\item \textbf{[5 pts]} 基于上一小问的结果，试证明下述SVM的泛化误差界
	\begin{equation}
		\mathbb{E}_{S \sim \mathcal{D}^m}[ R(h_S) ] \leq \mathbb{E}_{S \sim \mathcal{D}^{m+1}} \left[ \frac{N_{SV}(S)}{m+1} \right] , 
	\end{equation}
	其中$N_{SV}(S)$为$h_S$支持向量的个数。
\end{enumerate}

\begin{solution}
	~\\
	\begin{enumerate}[(1)]
		\item 
		\begin{align*}
			\mathbb{E}_{S\sim\mathcal{D}^m}[\hat{R}_{\text{LOO}}(\mathcal{A})]
			&=\mathbb{E}_{S\sim\mathcal{D}^m}[\mathbb{E}_{x\in S}[1_{h_{S-\{x\}}(x)\neq y}]]\\
			&=\mathbb{E}_{S\sim\mathcal{D}^m,x\in S}[1_{h_{S-\{x\}}(x)\neq y}]\\
			&=\mathbb{E}_{S'\sim\mathcal{D}^{m-1},x\sim D}[1_{h_{S'}(x)\neq y}]\\
			&=\mathbb{E}_{S'\sim\mathcal{D}^{m-1}}[\mathbb{E}_{x\sim D}[1_{h_{S'}(x)\neq y}]]\\
			&=\mathbb{E}_{S'\sim\mathcal{D}^{m-1}}[R(h_{S'})]
		\end{align*}
		\item 
		因为$h_{S-\{x\}}=h_S$，并且仅考虑可分情形，所以$h_{S-\{x\}}$会将$x$正确分类。逆反命题是如果$h_{S-\{x\}}$将$x$错误分类，那么$x$是$h_S$的支持向量。
		\item
		如果$x$不是$h_S$的支持向量，$h_{S-\{x\}}$会将$x$正确分类；如果$x$是$h_S$的支持向量，$h_{S-\{x\}}$对$x$分类可能正确也可能错误。所以分类错误个数少于支持向量个数。
		\begin{align*}
		\mathbb{E}_{S\sim\mathcal{D}^m}[R(h_S)]
		&=\mathbb{E}_{S\sim\mathcal{D}^{m+1}}[\hat{R}_{\text{LOO} }(\mathcal{A})]\\
		&=\mathbb{E}_{S\sim\mathcal{D}^{m+1}}\left[\frac{1}{m+1} \sum_{i=1}^{m+1} 1_{h_{S-\{x_i\}}(x_i)\neq y_i}\right]\\
		&\leq \mathbb{E}_{S\sim\mathcal{D}^{m+1}}\left[ \frac{N_{SV}(S)}{m+1}\right]
		\end{align*}
	\end{enumerate}
\end{solution}

\section*{[20 pts] Problem 4 [NN in Practice]}

\textbf{请结合编程题指南进行理解}
\par 在训练神经网络之前，我们需要确定的是整个网络的结构，在确定结构后便可以输入数据进行端到端的学习过程。考虑一个简单的神经网络：输入是2维向量，隐藏层由2个隐层单元组成，输出层为1个输出单元，其中隐层单元和输出层单元的激活函数都是$Sigmoid$函数。请打开\textbf{main.py}程序并完成以下任务：
\begin{enumerate}[(1)]
	\item \textbf{[4 pts]} 请完成Sigmoid函数及其梯度函数的编写。
	\item \textbf{[2 pts]} 请完成MSE损失函数的编写。
	\item \textbf{[9 pts]} 请完成NeuralNetwork\_221()类中train函数的编写，其中包括向前传播(可参考predict函数)、梯度计算、更新参数三个部分。
	\item \textbf{[5 pts]} 请对测试集(test\_feature.csv)所提供的数据特征完成尽量准确的分类预测。
\end{enumerate}

\begin{solution}
	~\\
	\begin{enumerate}[(1)]
		\item 
		Sigmoid的函数$f(x)=\frac{1}{1+e^{-x}}$，其导数$f'(x)=f(x)(1-f(x))$。
		\item 
		MSE损失函数$L(y,\hat{y})=\frac{1}{m}\sum_{i=1}^{m}(\hat{y}_i-y_i)^2$。
		\item 
		前向传播过程是，$h_1=sigmoid(w_1x_1+w_2x_2+b_1)$，$h_2=sigmoid(w_3x_1+w_4x_2+b_2)$，$ol=sigmoid(w_5h_1+w_6h_2+b_3)$。\\
		反向传播先计算$\frac{\partial L}{\partial ol}$，这与损失函数$L$的形式有关，对于MSE为$\hat{y}-y$。然后计算$\frac{\partial ol}{\partial w_5}=\frac{\partial ol}{\partial sum\_ol}\frac{\partial sum\_ol}{\partial w_5}=ol(1-ol)h_1$，同理$\frac{\partial ol}{\partial w_6}=ol(1-ol)h_2$，$\frac{\partial ol}{\partial b_3}=ol(1-ol)$。接着计算$\frac{\partial ol}{\partial h_1}=ol(1-ol)w_5$，$\frac{\partial ol}{\partial h_2}=ol(1-ol)w_6$。这时，再求$\frac{\partial h_1}{\partial w_1}$等的过程就与求$\frac{\partial ol}{\partial h_1}$类似，不再赘述。得到了每一层的输出对输入的偏导，只需使用链式法则相乘，即得损失对参数的偏导，再乘学习率，就是每轮更新的大小。
		\item 
		笔者调整了学习率，使用交叉验证确定早停轮数。同时还尝试了交叉熵损失，但效果并未显著提升，最终还是使用MSE损失。
	\end{enumerate}
\end{solution}

\end{document}